<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>VRD-IU 2024 - The 2024 Competition on Visually Rich Document Intelligence and Understanding</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<meta property="og:description" content=""/>
		<meta property="og:url" content="https://doc-iu.github.io/"/>
		<meta property="og:image:width" content="1200" />
		<meta property="og:image:height" content="627" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="#intro">DocIU Competition</a></li>
							<li><a href="#one">Overview</a></li>
							<li><a href="#two">Track A Challenge</a></li>
							<li><a href="#three">Track B Challenge</a></li>
							<li><a href="#four">Important Dates</a></li>
							<li><a href="#five">Organising Committee</a></li>
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

			<!-- Intro -->
				<section id="intro" class="wrapper style1 fullscreen fade-up">
					<div class="inner">
						<h1>The Competition on Visually Rich Document Intelligence and Understanding (VRD-IU)</h1>
						<p>The 2024 Competition on Visually Rich Document Intelligence and Understanding (VRD-IU) will be held in conjunction with 
						<a href="https://ijcai24.org/" target="_blank">the 33rd International Joint Conference on Artificial Intelligence (IJCAI 2024)</a> in Jeju Island, Korea. </p>
						<ul>
							<li>Competition date & time: <b>03.08.24 - 09.08.24 TBA</b> </li>
							<li>Competition physical location : <b><a href="https://ijcai24.org/">Jeju, South Korea</a></b>
							<li>Hybrid mode : <b><a href="">TBA</a></b></li>
						</ul>
					</div>
				</section>

			<!-- One -->
				<section id="one" class="wrapper style2 fade-up">
					<div class="inner">
						<h2 class="major">Overview</h2>
						<p> The VRD-IU(Visually Rich Document Intelligence and Understanding) competition aims to tackle the obstacles presented by the diverse and complex nature of form-like documents, which frequently involve multiple stakeholders and contain essential information that is challenging to extract. This competition, based on the Form-NLU Dataset featuring digital, printed, and handwritten forms, offers two tracks catering to participants' varying skill levels. Tasks range from extracting key information (Track A) to localising it within documents (Track B), ensuring engagement across proficiency levels in advancing visually rich document understanding technology. This initiative not only accelerates advancements in document understanding but also aims to draw increased interest and engagement in this field, presenting a prime opportunity for innovators to contribute to the evolution of efficient information extraction and analysis methodologies.</p>
						
					</div>
				</section>

			<!-- Two -->
				<section id="two" class="wrapper style3 fade-up">
					<div class="inner">
						<h2>Track A - Form Key Information Extraction</h2>
						Users must develop a deep learning-based retriever to extract the target form components based on the given key query. 
						We provide human-annotated semantic entities bounding box coordinates of input form documents; 
						users are required to locate the entity based on the input query.
						The evaluation metric is F1-Score following the Form-NLU Task B.</br>
						
						<b>Competition Link: <a href="https://www.kaggle.com/competitions/vrd-iu2024-tracka">https://www.kaggle.com/competitions/vrd-iu2024-tracka</a></b>
					</div>
				</section>

			<!-- Three -->
				<section id="three" class="wrapper style1 fade-up">
					<div class="inner">
						<h2>Track B - Form Key Information Localisation</h2>
						Users are encouraged to develop an end-to-end framework to predict the bounding box coordinates from the input document image based on the input key. 
							For Track B, no ground truth bounding box of form semantic entities is given; the inputs are only strictly formed images and key queries. 
							The evaluation metric is the Mean Average Precision (MAP) of the predicted bounding box.</p>
						<b>Competition Link: <a href="https://www.kaggle.com/competitions/vrd-iu2024-trackb">https://www.kaggle.com/competitions/vrd-iu2024-trackb</a></b>
					</div>
				</section>

			<!-- Four -->
				<section id="four" class="wrapper style2 fade-up">
					<div class="inner">
						<h2>Important Dates</h2>
								<ul>
									<li>Data, baseline paper & code available: <b>29 April, 2024</b></li>
									<li>Track A Challenge Due: <b>17 July, 2024</b></li>
									<li>Track B Challenge Due: <b>22 July, 2024</b></li>
									<li>Announcement of Winners: <b>24 July, 2024</b></li>
									<li>Paper Submission Due: <b>31 July, 2024</b></li>
									<li>Competition: <b>05 August, 2024</b></li>
									<li>Note: All deadlines are Anywhere on Earth (UTC - 12) time.</li>

								</ul>
					</div>
				</section>
			


			<!-- Five -->
				<section id="four" class="wrapper alt fade-up">
					<div class="inner">
						<h2>Organising Committee</h2>
						

<ul>
    <li><b>Organising Committee:</b>
        <ul>
            <li>Caren Han, The University of Melbourne</li>
            <li>Yihao Ding, The University of Sydney</li>
            <li>Yan Li, The University of Sydney</li>
            <li>Luca Cagliero, Politecnico di Torino</li>
            <li>Seong-Bae Park, Kyung Hee University</li>
            <li>Prasenjit Mitra, The Pennsylvania State University</li>
        </ul>
    </li>
    <li><b>Advisory Committee:</b>
        <ul>
            <li>Josiah Poon, The University of Sydney</li>
            <li>EJ Holden, The University of Melbourne</li>
        </ul>
    </li>
    <li><b>Program Committee:</b>
        <ul>
            <li>Haiqin Yang, International Digital Economy Academy, The Chinese University of Hong Kong, China</li>
            <li>Paolo Garza, Politecnico di Torino, Italy</li>
            <li>Honghan Wu, University College London, U.K.</li>
            <li>Riza Batista-Navarro, University of Manchester, U.K.</li>
            <li>Jean Lee, The University of Sydney, Australia</li>
            <li>Siwen Luo, The University of Western Australia, Australia</li>
            <li>Changyong Zhang, The University of Science and Technology of China, China</li>
            <li>Roberto Navigli, Sapienza University of Rome, Italy</li>
            <li>Lorenzo Vaiiani, Politecnico di Torino, Italy</li>
            <li>Davide Napolitano, Politecnico di Torino, Italy</li>
            <li>HeeGuen Yoon, National Information Society Agency, Korea</li>
            <li>So-Eon Kim, Kyung Hee University, Korea</li>
            <li>Nianlong Gu, University of Zurich, Switzerland</li>
            <li>Yingqiang Gao, University of Zurich and ETH Zurich, Switzerland</li>
            <li>Ali Rasekh, L3S Research Center, Germany</li>
        </ul>
    </li>
</ul>
						<p>For any queries, send an email to <a href="mailto:caren.han@unimelb.edu.au">caren.han@unimelb.edu.au</a></p>
					</div>
				</section>
			</div>


		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
